# GCS-Infra-Live ES -> [EN](README.en.md)


# Indice de contenidos
<!-- toc -->

- [Infra usando y horneando imagen Packer.](#infra-usando-y-horneando-imagen-packer)
  - [Workflow Packer + Terraform (feat/dev)](#workflow-packer-terraform-featdev)
  - [Imagen Packer horneada con k3s](#imagen-packer-horneada-con-k3s)
- [Infra Apply + Ansible (post-apply) en rama main](#infra-apply--ansible-post-apply-en-rama-main)
  - [Estado de salud del entorno](#Estado-de-salud-del-entorno)
  - [Qu√© se a√±adi√≥](#qu√©-se-a√±adi√≥)
  - [Requisitos](#requisitos)
  - [Estructura recomendada del repositorio (En proceso)](#estructura-recomendada-del-repositorio-en-proceso)
  - [C√≥mo funciona el pipeline completo](#c√≥mo-funciona-el-pipeline-completo)
  - [Estructura de carpetas en la VM](#estructura-de-carpetas-en-la-vm)
  - [Stack desplegado](#stack-desplegado)
  - [Ansible site.yml](#ansible-siteyml)
  - [Stack Docker: docker-compose.yml](#Stack-Docker-docker-compose-yml)
  - [Conexion a la VM usando IAP y VS Code o cualquier otra herramienta.](#conexion-a-la-vm-usando-iap-y-vs-code-o-cualquier-otra-herramienta)
  - [Buenas pr√°cticas que hemos seguido](#buenas-pr√°cticas-que-hemos-seguido)
<!-- tocstop -->

# Infra usando y horneando imagen Packer.

En esta parte del documento se muestra la estructura y la creacion de recursos usando una plantilla packer y archivos .tf los cuales son generados al hacer un `pull_request` desde la rama de prueba a la Rama `main`. Cabe destacar que la verificacion de `packer`ya se hace en la propia rama de pruebas `feat/dev`.

La infraestructura usada a traves del packer es solo como aprendizaje y conocimiento, debido a la complejidad de dicha parte del proyecto, se mejorara y se seguira poco a poco.

## Workflow Packer + Terraform (feat/dev)
- Workflow: `.github/workflows/feat-dev-packer-net-plan.yaml`.
- Orden: Packer (init/fmt/validate) ‚Üí Terraform (fmt/init/validate/plan) en `environments/packer-dev/terraform-net` (VPC, subred privada, NAT, firewall IAP y VM sin IP p√∫blica basada en la imagen de Packer).
- Imagen: familia `ubuntu-2204-iap-family` publicada por Packer en el proyecto `TF_VAR_project_id`.
- Variables: `GCP_INFRA_PROJECT_ID/REGION/ZONE/NETWORK/SUBNETWORK`, `GCP_VM_SERVICE_ACCOUNT` y SA de Packer (`GCP_PACKER_SERVICE`) se pasan como variables del repo.
- Apply final: se hace en el workflow de `main` (manual y con entorno protegido); en feat/dev solo se valida plan y Packer.

## Imagen Packer horneada con k3s
- Base: Ubuntu 22.04, sin IP p√∫blica, acceso por IAP, tags `iap-ssh`/`packer-dev`.
- K3s `v1.34.1+k3s1` instalado pero detenido (`INSTALL_K3S_SKIP_START=true`); config base en `/etc/rancher/k3s/config.yaml` (traefik/servicelb desactivados, flannel vxlan, CIDRs 10.42/10.43, `node-name: k3s-server-1`, kubeconfig 0644).
- Token **no** embebido: se genera en runtime al arrancar el server.
- Paquetes pinneados y hold (`curl`, `git`, `ca-certificates`, `jq`); cach√© APT limpia; imagen publicada en la familia `ubuntu-2204-iap-family`.
- Autenticaci√≥n: Packer utiliza una `SA`diferente que la de `Terraform`para separar, tambien usa `IAP` + `firewall` para limitar el acceso.
- OS Login ahora mismo esta en `false`, pero mas adelante se puede activar sin problemas.
- Para consumir la imagen se indica la familia de esta `local.packer_image_family` se usa en `boot_disk`, si la imagen su publica en otro proyecto, se necesitaria cambiar el `project` en la misma ruta.

# Infra Apply + Ansible (post-apply) en rama main

Este documento resume los cambios realizados en main para ejecutar configuraciones de Ansible de forma segura despu√©s del terraform apply, usando `OS Login + IAP` (sin llaves SSH ni puerto 22 p√∫blico) y un inventario generado `on-the-fly.`

## üîç Estado de salud del entorno

[![Health report](https://github.com/S4M73l09/GCS-Infra-Live/actions/workflows/health-report.yml/badge.svg)](https://github.com/S4M73l09/GCS-Infra-Live/actions/workflows/health-report.yml)

Esta linea es un ejemplo de justo utilizar `Python` para crear un sistema de alertas y reports que se guardan de normal en los artifacts de Github, esto no esta actualmente puesto debido a que al usar tunel IAP para la conexion de VM, es imposible ponerlo para que se conecte.

## Qu√© se a√±adi√≥
### 1) Workflow encadenado: Inventory-And-Ansible.yaml
* Ubicaci√≥n: `.github/workflows/Inventory-And-Ansible.yaml`
* Se dispara autom√°ticamente cuando el workflow `terraform-apply` finaliza con √©xito.
* Hace 3 jobs separados (visibilidad ‚Äúpro‚Äù en Actions):
   #### *1) generate_inventory*
     * Autentica en GCP v√≠a OIDC.
     * Genera ansible/hosts.ini con las VMs RUNNING que tengan `labels.env=entorno (por defecto dev)`.
     * Crea `ansible/ansible.cfg` apuntando a `~/.ssh/config (gcloud + IAP)`.
     * Verifica que el inventario no contenga IPs (solo FQDN GCE).
     * Sube ambos ficheros como `artifact: ansible-inventory-env`.
   #### *2) publish_inventory*
     * Publica/eco del artifact listo (opcional, solo visibilidad).
   #### *3) run_ansible*
     * Descarga el artifact en `ansible/.`
     * Instala Ansible.
     * Ejecuta `ansible-playbook ansible/site.yml` usando `ANSIBLE_CONFIG=ansible/ansible.cfg`.  


#### Si queremos cubrir prod, a√±adimos prod a la matrix.env en los tres jobs.
---

### 2) Playbook m√≠nimo de Ansible: ansible/site.yml
* Crea la estructura de carpetas en la VM bajo `/opt/monitoring`.
* Copia (si existen en el repo) los archivos de `Prometheus, Alertmanager, Grafana y Docker` hacia la VM.
---
## Requisitos

### Variables de repositorio (Settings ‚Üí Variables)
* `GCP_WORKLOAD_IDENTITY_PROVIDER (WIF)`
* `GCP_SERVICE_ACCOUNT` (SA usado por OIDC)

### IAM necesarias para el identity que ejecuta los workflows
* `roles/compute.osAdminLogin` (o roles/compute.osLogin si no se necesita sudo)
* `roles/iap.tunnelResourceAccessor`

### Red/Firewall
* SSH solo por `IAP`
* OS Login activado (a nivel proyecto y/o VM): `enable-oslogin = TRUE`.

### Etiquetas
* Las VMs que deben entrar en el inventario deben tener `labels.env=dev` (o el entorno que se use).

## Estructura recomendada del repositorio (En proceso)
```bash
environments/dev                     # (Ya a√±adido por mergear a main)
ansible/
  site.yml
  requirements.yml
  web/
    index.html
    style.css
  files/
    monitoring/
      prometheus/
        prometheus.yml                # (ya puesto)
        rules/
          alerts.yml                  # (ya puesto)
      grafana/
        provisioning/
          datasources/
            datasource.yml            # (ya puesto)
          dashboards/
            ejemplo.json              # (ya puesto)
    template/
      monitoring/
        alertmanager.yml.j2         # (Plantilla que generara el Alertmanager.yml)
        docker-compose.yml.j2       # (Plantilla para docker-compose.yml)
README.md
README.en.md
renovate.json
```
---
## C√≥mo funciona el pipeline completo
1. Se hace merge a main y corre el `terraform-apply` (con revisi√≥n del environment si procede).
2. Al terminar OK, se lanza `inventory-and-ansible`:
   * **Job 1:** genera inventario por etiquetas y sube artifact.
   * **Job 2:** marca visibilidad del artifact (opcional).
   * **Job 3:** descarga el artifact en `ansible/` e invoca `ansible-playbook` contra tus hosts.
   * **Job 4:** Instala la coleccion necesaria de Ansible `community.docker`.

3. Despues de la ejecucion del pipeline `inventory-and-ansible`.

   - Instala Docker y el plugin de Docker Compose en la VM.
   - Copia el stack de monitoring a `/opt/monitoring`.
   - Renderiza la configuraci√≥n de Alertmanager desde una plantilla usando **GitHub Secrets**.
   - Levanta (o actualiza) el stack con `docker compose` de forma idempotente.

## Estructura de carpetas en la VM

```makefile
/opt
‚îú‚îÄ monitoring/
‚îÇ  ‚îú‚îÄ docker/                       # directorio de trabajo del compose
‚îÇ  ‚îÇ  ‚îî‚îÄ docker-compose.yml         # generado por Ansible
‚îÇ  ‚îú‚îÄ prometheus/
‚îÇ  ‚îÇ  ‚îú‚îÄ prometheus.yml             # bind: ../prometheus/prometheus.yml -> /etc/prometheus/prometheus.yml
‚îÇ  ‚îÇ  ‚îî‚îÄ rules/                     # bind: ../prometheus/rules -> /etc/prometheus/rules
‚îÇ  ‚îú‚îÄ alertmanager/
‚îÇ  ‚îÇ  ‚îî‚îÄ alertmanager.yml           # bind: ../alertmanager/alertmanager.yml -> /etc/alertmanager/alertmanager.yml
‚îÇ  ‚îî‚îÄ grafana/
‚îÇ     ‚îî‚îÄ provisioning/              # bind: ../grafana/provisioning -> /etc/grafana/provisioning
‚îÇ
‚îú‚îÄ web01/                           # bind: /opt/web01 -> /usr/share/nginx/html (servicio web)
‚îÇ   ‚îú‚îÄ index.html
‚îÇ   ‚îî‚îÄ style.css
‚îÇ
‚îî‚îÄ volumes (Docker-managed):
    ‚îú‚îÄ prometheus-data              # volumen nombrado (datos de Prometheus)
    ‚îî‚îÄ grafana-data                 # volumen nombrado (datos de Grafana)
```


## Stack desplegado:

- `prometheus`
- `alertmanager`
- `node-exporter`
- `web (hosting simple)`
- `grafana` (con datasource de Prometheus preconfigurado)
- Reglas de alertas b√°sicas + salud del host
- Alertas enviadas por correo v√≠a Alertmanager

## Ansible site.yml

El playbook encargado de:

1. Instalar Docker + plugin de compose.

2. Crear directorios:
   * `monitoring_base_dir: /opt/monitoring`
   * `web01_base_dir: /opt/web01`

3. Copiar:
   * `docker-compose.yml` y ficheros de Prometheus, Alertmanager y Grafana a `/opt/monitoring`.
   * Contenido de `files/web/` (HTML/CSS) a `/opt/web01`.

4. Levantar o actualizar el stack:
```yaml
community.docker.docker_compose_v2:
  project_src: "{{ monitoring_base_dir }}"
  state: present
  remove_orphans: true
```

Cuando cambian los archivos de la web o del stack, se notifica al handler `restart monitoring stack` para recrear los contenedores.

## üê≥ Stack Docker: docker-compose.yml

Servicios desplegados en el `docker-compose.yml`


Ruta: `ansible/templates/monitoring/docker-compose.yml.j2`

En esta plantilla se usa justo lo necesario para el despliegue, se usa plantilla para que se puedan meter las variables de usuario de Grafana para mejor seguridad.

* `prometheus` -> recolecta m√©tricas 
* `alertmanager` -> gestiona las alertas
* `node-exporter` -> m√©tricas de sistema (CPU, RAM, disco, red)
* `grafana` -> visualizacion de metricas
* `web (Nginx)` -> sirve la web est√°tica desde `/opt/web01` 
* `blackbox` -> Comprueba la disponibilidad HTTP de la web


Ademas a√±ade buenas practicas: 

**Prometheus**

- Imagen `prom/prometheus`.
  - Expone `9090`.
  - Monta configuraci√≥n y reglas como vol√∫menes **de solo lectura**.
  - Persiste datos en el volumen `prometheus-data`.
  - Healthcheck HTTP (`/-/healthy`).
  - L√≠mites de recursos: ~0.5 CPU y 1 GB de RAM.

**Alertmanager**

- Imagen `prom/alertmanager`.
  - Expone `9093`.
  - Configuraci√≥n montada como volumen **ro** desde `./alertmanager/alertmanager.yml`.
  - Healthcheck HTTP (`/-/healthy`).

**Node exporter**

- Imagen `prom/node-exporter`.
  - Expone `9100`.
  - Monta `/proc`, `/sys` y `/` en **solo lectura** para la recolecci√≥n de m√©tricas.
  - Comando ajustado para usar rutas host (`--path.rootfs`, etc.).

**grafana**

- Imagen `grafana/grafana-oss` (a fijar a versi√≥n estable).
  - Expone `3000`.
  - Healthcheck HTTP (`/api/health`).
  - Admin user/password se inyectan desde **secrets de GitHub** a trav√©s de Ansible:
    - `GF_SECURITY_ADMIN_USER="{{ lookup('env', 'GRAFANA_ADMIN_USER') | default('admin') }}"`
    - `GF_SECURITY_ADMIN_PASSWORD="{{ lookup('env', 'GRAFANA_ADMIN_PASSWORD') | default('admin') }}"`
  - Desactiva el registro de nuevos usuarios (`GF_USERS_ALLOW_SIGN_UP=false`).
  - Persistencia en el volumen `grafana-data`.
  - Provisioning montado desde `./grafana/provisioning`.

**Web (Nginx)**

- Imagen `nginx:alpine`.
  - Expone `80`.
  - Sirve el portfolio est√°tico desde `/opt/web01` en la VM (montado **ro**).

**Blackbox**

- Imagen `prom/blackbox-exporter`.
  - Expone `9115`.
  - Healthcheck HTTP (`/-/healthy`).
  - Se conecta tanto a la red de monitoring como a la red de la web para poder sondear el portfolio.

### Redes y volumenes

- Redes:
  - `monitoring`: Prometheus, Alertmanager, Node Exporter, Grafana, Blackbox.
  - `web-01`: Nginx (web) y Blackbox.

- Vol√∫menes:
  - `prometheus-data`: datos de Prometheus.
  - `grafana-data`: datos de Grafana.


### Buenas practicas aplicadas a la plantilla de docker-compose.yml.j2

- **Separaci√≥n de responsabilidades**: cada servicio en su contenedor (Prometheus, Alertmanager, Node Exporter, Grafana, Nginx, Blackbox).
- **Seguridad**:
  - Credenciales de Grafana gestionadas v√≠a **GitHub Secrets + Ansible + plantilla Jinja2**, nunca en el repo.
  - Vol√∫menes de configuraci√≥n montados en modo **read-only**.
  - `security_opt: no-new-privileges:true` en los servicios para reducir escalado de privilegios.
- **Recursos controlados**:
  - L√≠mites de CPU y memoria para Prometheus y Grafana (`deploy.resources.limits`) para evitar que saturen la VM.
- **Gesti√≥n de logs**:
  - Driver `json-file` con rotaci√≥n (`max-size: 10m`, `max-file: 3`) para no llenar el disco.
- **Observabilidad del propio stack**:
  - Healthchecks HTTP en Prometheus, Alertmanager, Grafana y Blackbox para detectar r√°pidamente estados no saludables.
- **Variables lookup('env')**:
  - Variables puestas en el workflow usando `Lookup('env')` para inyectarlas sin mostrar en las plantillas seleccionadas, mejorando la seguridad.


## üìä Prometheus: prometheus.yml + reglas

Ruta: `ansible/files/monitoring/prometheus/prometheus.yml`.

Scrapea:

* prometheus:9090

* alertmanager:9093

* node-exporter:9100

Carga reglas desde `/etc/prometheus/rules/*.yml`.

***Env√≠a alertas a Alertmanager.***

### Reglas de alertas

Ruta: `ansible/files/monitoring/prometheus/rules/alerts.yml`.

Incluye dos grupos de reglas:

* `infra_basic`: estado general de targets, Prometheus y Alertmanager.
* `host_health`: salud del host basada en m√©tricas de `node_exporter` (**CPU**, **memoria**, **disco**, **filesystem read-only**, etc.).

* Ademas de a√±adir `blackbox` para la monitorizacion de la pagina web.  
   * blackbox interno:  
     * Target: `http://web:80`  
   * blackbox externo o publico(Cuando haya dominio + cloudflare):  
     * Target: `https://Nuestro-dominio.com`  

Ambos jobs usan `relabel_configs` para enviar las peticiones al `blackbox-exporter` en `blackbox:9115`.

## üìà Grafana: datasource provisioning

Ruta: `ansible/files/monitoring/grafana/provisioning/datasources/datasource.yml`.

Datasource de Prometheus creado autom√°ticamente al arrancar Grafana.

Ademas, Grafana usa `secrets and variables` para almacenar el usuario y la contrase√±a de este para mayor seguridad

## üì¨ Alertmanager: plantilla con SMTP y GitHub Secrets

Ruta: `ansible/templates/monitoring/alertmanager.yml.j2`

Alertmanager se configura a partir de una plantilla Jinja2.

Las credenciales SMTP se inyectan desde GitHub Secrets en el workflow de Ansible:

### Secrets en GitHub necesarios  
En Settings ‚Üí `Secrets and variables` ‚Üí `Actions`:

* `ALERT_SMTP_SMARTHOST` ‚Üí ej. smtp.gmail.com:587

* `ALERT_SMTP_FROM` ‚Üí correo remitente

* `ALERT_SMTP_USER` ‚Üí usuario SMTP (normalmente el mismo correo)

* `ALERT_SMTP_PASS` ‚Üí contrase√±a de aplicaci√≥n del proveedor de correo

* `ALERT_SMTP_TO` ‚Üí correo destino (si se omite, usa ALERT_SMTP_FROM)

* `GRAFANA_ADMIN_USER` ‚Üí Nombre de usuario de Grafana.

* `GRAFANA_ADMIN_PASSWORD` ‚Üí Contrase√±a de administrador de Grafana.

El step de Ansible en el workflow pasa estos secrets al playbook como variables `lookup('env', '...')`, que la plantilla usa para generar `alertmanager.yml` en la VM y las variables de Grafana en el archivo de `docker-compose.yml`.

## Web est√°tica

* Nginx sirve una p√°gina simple de portfolio/explicacion del proyecto:
   * Descripcion de la infraestructura (Terraform + GCP + Github Action + Ansible + Docker)
   * Embeds de video de youtube mostrando el Bootstrap/repo-Live de la infraestructura
   * Enlaces a los repositorios de ***Bootstrap*** y ***Infra-Live***
* El contenido HTML/CSS vive en el repo bajo `ansible/web` y se copia a `/opt/web01` mediante Ansible.

## Artifacts y visibilidad 

En cada run del workflow de `Ansible` al final de todo genera un artifact que se puede descargar el cual muestra el comando directo que se puede usar para conectarse a la VM usando su nombre real y demas utilizamdo `IAP-Tunnel`.

## Validac√≠on local (WSL)

Herramientas usadas para validar la configuracion antes de desplegar:

*  Ansible  
   * `ansible-playbook site.yml --syntax-check`

* Prometheus
   * `promtool check config files/monitoring/prometheus/prometheus.yml`
   * `promtool check rules files/monitoring/prometheus/rules/alerts.yml`

* Docker-compose
   * `docker-compose config` (desde `files/monitoring/docker`)

* YAML linting
   * `yamllint` sobre `prometheus.yml` y `alerts.yml` para limpiar espacios y comentarios

## Flujo de promoci√≥n de Terraform (`feat/dev` ‚Üí `main`)

En este repo usamos las ramas as√≠:

- `main` ‚Üí rama **LIVE / producci√≥n**.  
- `feat/dev` ‚Üí rama de **desarrollo y pruebas** (Terraform, workflows, README, etc.).  
- Ramas tipo `feat/tf-...` ‚Üí ramas **temporales de promoci√≥n**, solo para llevar cambios de Terraform desde `feat/dev` a `main`.

La idea es:

> En `feat/dev` puedo tocar de todo.  
> A `main` solo llegan los cambios de Terraform que yo decido, a trav√©s de una rama de promoci√≥n.

---

### Pasos para subir cambios de Terraform a `main`

Supongamos que los `.tf` est√°n en `environments/dev`.

1. **Trabajar normal en `feat/dev`**

   - Editar Terraform en `environments/dev`.
   - Probar, validar, hacer `terraform plan`, etc.
   - Hacer commits y `git push` a `feat/dev` hasta que la infra est√© lista.

2. **Cuando los cambios de Terraform est√©n listos para producci√≥n**

   Crear una rama de promoci√≥n limpia desde `main`:

   ```bash
   # 1) Ir a main y actualizar
   git checkout main
   git pull origin main

   # 2) Crear rama de promoci√≥n (nombre de ejemplo)
   git checkout -b feat/tf-update-<descripcion>

Traer solo la carpeta de Terraform desde feat/dev:
```
git checkout feat/dev -- environments/dev
```

Comprobar qu√© ha cambiado:
```bash
git status
```
‚Üí Aqu√≠ solo deber√≠an aparecer archivos bajo `environments/dev`.

Hacer commit y subir la rama:
```bash
git add environments/dev
git commit -m "Actualizar Terraform desde feat/dev"
git push origin feat/tf-update-<descripcion>
```

3. **Crear el pull_request a `main`**

   - Abrir un PR: `feat/tf-update-description -> main`
   - Revisar el diff: solo deben aparecer archivos dentro de `envinronments/dev`
   - Dejar que se ejecuten los workflows
   - Si todo esta OK -> hacer merge a `main`

4. **Limpiar rama de promocion**

Una vez mergeado el PR:

```bash
# Borrar la rama local
git branch -d feat/tf-update-<descripcion>

# Borrar la rama en remoto (GitHub)
git push origin --delete feat/tf-update-<descripcion>
```

## ¬øPor qu√© usamos directamente `feat/dev -> main`

El PR `feat/dev -> main` arrastrar√≠a todos los cambios de esa rama(README, workflows, pruebas, etc.), no solo Terraform.

Con este flujo: 

* `feat/dev` sigue siendo un **taller** donde se pueda tocar de todo.
* `main` solo recibe, mediante ramas de promocion (`feat/tf-...`), los cambios de Terraform que ya estan listos para la produccion.

## Artefacts y visibilidad
* Inventario y cfg quedan guardados como artifact:  
`ansible-inventory-env` ‚Üí `ansible/hosts.ini`, `ansible/ansible.cfg`

* Puedes descargarlo desde la pesta√±a ***Actions*** del run correspondiente.

## Acceso a VM sin IP p√∫blica con IAP + VS Code Remote-SSH

Esta VM (`dev-oslogin-ubuntu`, `europe-west1-b`) **no tiene IP p√∫blica**.  
El acceso se hace solo por:

- **IAP (Identity-Aware Proxy)**  
- **SSH** con `ProxyCommand` que llama a `gcloud` en WSL  
- **VS Code Remote-SSH**  
- **Port forwarding** para Nginx / Prometheus / Grafana

---

## Conexion a la VM usando IAP y VS Code o cualquier otra herramienta.

## 1. Requisitos

### Local (Windows + WSL)

- Windows 10/11 con **OpenSSH Client**.
- **VS Code** + extensi√≥n **Remote - SSH**.
- **WSL2 Ubuntu** con:
  - Google Cloud SDK (`gcloud`) instalado.
  - Proyecto configurado:
    ```bash
    gcloud auth login
    gcloud config set project NAME-Project
    ```
  - Clave creada por `gcloud`:
    `/home/USUARIO/.ssh/google_compute_engine`

Copiar la clave de WSL a Windows:

```bash
mkdir -p /mnt/c/Users/USUARIO/.ssh
cp /home/USUARIO/.ssh/google_compute_engine \
   /mnt/c/Users/USUARIO/.ssh/google_compute_engine
```

### GCP

 - Proyecto: `Nombre del proyecto`
 - VM: `nombre-de-lav`
 - IAP habilitado + permisos para la cuenta que se conecta.

## 2. Configuracion de la VM

Archivo: `C:\Users\USUARIO\.ssh\config`

```sshconfig
Host gcp-dev-iap
    HostName compute.Numerosoltadoporcomandodryrun
    User USUARIO

    IdentityFile C:/Users/USUARIO/.ssh/google_compute_engine
    IdentitiesOnly yes

    # T√∫nel IAP usando gcloud en WSL
    ProxyCommand wsl /home/USUARIO/google-cloud-sdk/platform/bundledpythonunix/bin/python3 /home/USUARIO/google-cloud-sdk/lib/gcloud.py compute start-iap-tunnel nombre-de-la-vm %p --listen-on-stdin --project=nombre-del-proyecto --zone=zona-ubicada-de-la-VM --verbosity=warning

    # T√∫neles HTTP locales
    LocalForward 8080 localhost:80      # Nginx / web
    LocalForward 9090 localhost:9090    # Prometheus
    LocalForward 3000 localhost:3000    # Grafana
```
> - HostName compute.NUMEROLARGO sale de
>   - gcloud compute ssh nombre-de-la-VM --tunnel-through-iap --dry-run.

## 3. Probar conexi√≥n SSH 

En powershell:
```powershell
ssh gcp-dev-iap
```

Si funciona deber√≠as ver:
```bash
Usuario@Nombre-de-la-VM:~$
```
Y mientras la sesi√≥n est√© abierta:

- `http://localhost:8080` ‚Üí Nginx / web

- `http://localhost:9090` ‚Üí Prometheus

- `http://localhost:3000` ‚Üí Grafana

## 4. Uso con VS Code Remote-SSH

  1. Abrir VS Code
  2. `Ctrl+Shift+P`
  3. Seleccionar `gcp-dev-iap`
  4. Esperar a que se instale el ***VS Code Server*** en la VM (Primera vez)
  5. Abajo a la izquierda debe poner: `SSH: gcp-dev-iap`.
  6. `Terminal -> New Terminal` -> prompt esperado
  ```bash
    USUARIO@Nombre-de-la-VM:~$
   ```
A partir de ahora: 

  - Se edita archivos directamente en la VM.
  - Usas la terminal remota para `docker`, logs, ect.
  - Accedes a los servicios v√≠a `localhost:8080/9090/3000`.

## 5. Notas r√°pidas

  - Si Remote-SSH se rompe:  
  en la VM -> `rm -rf ~/.vscode-server` y volver a conectar.

  - Aseg√∫remonos de tener `curl/wget` instalados en la VM para que VS Code pueda descargar el server:
  ```bash
  sudo apt update
  sudo apt install -y curl wget
  ```

Asi queda documentado el patr√≥n: ***VM sin IAP p√∫blica + IAP + VS Code + t√∫neles Locales***


## Buenas pr√°cticas que hemos seguido
* Sin llaves SSH ni 22 p√∫blico: acceso por IAP.
* Inventario ef√≠mero (on-the-fly) y sin IPs (solo FQDN GCE).
* Concurrencia en el workflow para evitar solapes.
* Jobs separados para m√°xima visibilidad (generate ‚Üí publish ‚Üí apply).
